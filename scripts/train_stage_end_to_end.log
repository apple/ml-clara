nohup: ignoring input
+ DEBUG=0
+ export PYTHONPATH=/mnt/ceph_rbd/comp_rag/unirag:
+ PYTHONPATH=/mnt/ceph_rbd/comp_rag/unirag:
+ export WANDB_DIR=/mnt/ceph_rbd/comp_rag/unirag/debug_data/wandb_logs
+ WANDB_DIR=/mnt/ceph_rbd/comp_rag/unirag/debug_data/wandb_logs
+ data_path=/mnt/ceph_rbd/comp_rag/unirag/debug_data
+ SAVE_MODEL_NAME=unirag_stage2_debug
+ SAVE_PATH=/mnt/ceph_rbd/comp_rag/unirag/debug_data/train_checkpoint/unirag_stage2_debug
+ WANDB_TOKEN=xx
+ MODEL_PATH=/mnt/ceph_rbd/model/Mistral-7B-Instruct-v0.2
+ mkdir -p /mnt/ceph_rbd/comp_rag/unirag/debug_data/train_checkpoint/unirag_stage2_debug
+ ARG_SCRIPT=distributed_arguments.py
+ NCCL_DEBUG=INFO
++ python distributed_arguments.py num_nodes
+ NUM_NODES=1
++ python distributed_arguments.py master
+ MASTER=127.0.0.1
++ python distributed_arguments.py port
+ MASTER_PORT=29500
++ python distributed_arguments.py rank
+ NODE_RANK=0
++ python distributed_arguments.py num_gpus
+ NUM_LOCAL_GPUS=4
+ WORLD_SIZE=4
+ echo 'Number of nodes: 1'
Number of nodes: 1
+ echo 'WORLD_SIZE: 4'
WORLD_SIZE: 4
+ echo 'Number of local GPUs: 4'
Number of local GPUs: 4
+ echo 'Master: 127.0.0.1'
Master: 127.0.0.1
+ echo 'Master port: 29500'
Master port: 29500
+ echo 'Node rank: 0'
Node rank: 0
++ which python
+ echo 'Currently using /mnt/ceph_rbd/conda_env/cprag/bin/python'
Currently using /mnt/ceph_rbd/conda_env/cprag/bin/python
+ training_commands='openrlhf.cli.train_sft    --max_len 1024    --dataset /mnt/ceph_rbd/comp_rag/unirag/debug_data/end_to_end_data.jsonl    --pretrain /mnt/ceph_rbd/model/Mistral-7B-Instruct-v0.2    --pretrain_checkpoint /mnt/ceph_rbd/comp_rag/unirag/debug_data/train_checkpoint/unirag_cluster1_2_2m_split_data_single_32_mistral    --train_batch_size 32    --micro_train_batch_size 2    --ckpt_path /mnt/ceph_rbd/comp_rag/unirag/debug_data/train_checkpoint/unirag_stage2_debug    --max_samples 500    --save_path /mnt/ceph_rbd/comp_rag/unirag/debug_data/train_checkpoint/unirag_stage2_debug    --save_steps -2    --logging_steps 1    --eval_steps 100    --zero_stage 2    --max_epochs 1    --bf16    --flash_attn    --learning_rate 5e-6    --stage stage2    --lr_scheduler constant    --generation_top_k 5    --qa_loss    --do_eval_gen    --doc_max_length 256    --compress_rate 32    --gradient_checkpointing'
+ DISTRIBUTED_ARGS='--nproc_per_node 4 --nnodes 1 --rdzv_id 101 --rdzv_backend c10d --rdzv_endpoint 127.0.0.1:29500 --master_addr 127.0.0.1 --master_port 29500 --node_rank 0'
+ echo 'Starting UniRAG stage2 training on node 0 of 1 nodes...'
Starting UniRAG stage2 training on node 0 of 1 nodes...
+ '[' 0 -eq 0 ']'
+ '[' 1 -gt 1 ']'
+ torchrun --nproc_per_node 4 --nnodes 1 --rdzv_id 101 --rdzv_backend c10d --rdzv_endpoint 127.0.0.1:29500 --master_addr 127.0.0.1 --master_port 29500 --node_rank 0 -m openrlhf.cli.train_sft --max_len 1024 --dataset /mnt/ceph_rbd/comp_rag/unirag/debug_data/end_to_end_data.jsonl --pretrain /mnt/ceph_rbd/model/Mistral-7B-Instruct-v0.2 --pretrain_checkpoint /mnt/ceph_rbd/comp_rag/unirag/debug_data/train_checkpoint/unirag_cluster1_2_2m_split_data_single_32_mistral --train_batch_size 32 --micro_train_batch_size 2 --ckpt_path /mnt/ceph_rbd/comp_rag/unirag/debug_data/train_checkpoint/unirag_stage2_debug --max_samples 500 --save_path /mnt/ceph_rbd/comp_rag/unirag/debug_data/train_checkpoint/unirag_stage2_debug --save_steps -2 --logging_steps 1 --eval_steps 100 --zero_stage 2 --max_epochs 1 --bf16 --flash_attn --learning_rate 5e-6 --stage stage2 --lr_scheduler constant --generation_top_k 5 --qa_loss --do_eval_gen --doc_max_length 256 --compress_rate 32 --gradient_checkpointing
W1028 21:47:09.134000 33342 site-packages/torch/distributed/run.py:792] 
W1028 21:47:09.134000 33342 site-packages/torch/distributed/run.py:792] *****************************************
W1028 21:47:09.134000 33342 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1028 21:47:09.134000 33342 site-packages/torch/distributed/run.py:792] *****************************************
============================================================
============================================================UniRAG Training Configuration

============================================================UniRAG Training Configuration============================================================

============================================================
UniRAG Training Configuration============================================================
Training stage: stage2

UniRAG Training Configuration
============================================================Training stage: stage2
Base model: /mnt/ceph_rbd/model/Mistral-7B-Instruct-v0.2

============================================================
Training stage: stage2Base model: /mnt/ceph_rbd/model/Mistral-7B-Instruct-v0.2
Document max length: 256

Training stage: stage2
Base model: /mnt/ceph_rbd/model/Mistral-7B-Instruct-v0.2Document max length: 256
Compression rate: 32

Base model: /mnt/ceph_rbd/model/Mistral-7B-Instruct-v0.2
Document max length: 256Compression rate: 32
Generation top-k: 5

Document max length: 256
Compression rate: 32Generation top-k: 5
Dataset: /mnt/ceph_rbd/comp_rag/unirag/debug_data/end_to_end_data.jsonl

Compression rate: 32
Generation top-k: 5Dataset: /mnt/ceph_rbd/comp_rag/unirag/debug_data/end_to_end_data.jsonl
Max epochs: 1

Generation top-k: 5
Dataset: /mnt/ceph_rbd/comp_rag/unirag/debug_data/end_to_end_data.jsonlMax epochs: 1

Learning rate: 5e-06
Dataset: /mnt/ceph_rbd/comp_rag/unirag/debug_data/end_to_end_data.jsonlMax epochs: 1

Learning rate: 5e-06
Batch size (micro/global): 2/32Max epochs: 1

Learning rate: 5e-06
Batch size (micro/global): 2/32============================================================


Learning rate: 5e-06Batch size (micro/global): 2/32============================================================


Batch size (micro/global): 2/32============================================================

============================================================
Loading model from checkpoint: /mnt/ceph_rbd/comp_rag/unirag/debug_data/train_checkpoint/unirag_cluster1_2_2m_split_data_single_32_mistralLoading model from checkpoint: /mnt/ceph_rbd/comp_rag/unirag/debug_data/train_checkpoint/unirag_cluster1_2_2m_split_data_single_32_mistralLoading model from checkpoint: /mnt/ceph_rbd/comp_rag/unirag/debug_data/train_checkpoint/unirag_cluster1_2_2m_split_data_single_32_mistral
Loading model from checkpoint: /mnt/ceph_rbd/comp_rag/unirag/debug_data/train_checkpoint/unirag_cluster1_2_2m_split_data_single_32_mistral


Initializing model from trained checkpoint: UniRAGConfig {
  "_attn_implementation_autoset": true,
  "ae_mode": "token",
  "attn_implementation": null,
  "auto_map": {
    "AutoConfig": "modeling_unirag.UniRAGConfig",
    "AutoModel": "modeling_unirag.UniRAG"
  },
  "compr_base_model_name": "/mnt/ceph_rbd/model/Mistral-7B-Instruct-v0.2",
  "compr_every_n_layer": null,
  "compr_linear_type": "concat",
  "compr_mlp_hidden_dim": 8096,
  "compr_model_name": null,
  "compr_n_layers": 5,
  "compr_rate": 32,
  "compr_rms_norm": false,
  "compr_use_mlp": false,
  "decoder_model_name": "/mnt/ceph_rbd/model/Mistral-7B-Instruct-v0.2",
  "device_map": null,
  "different_mem_tokens": true,
  "doc_max_length": 256,
  "generation_top_k": 5,
  "kbtc_training": false,
  "load_adapters": false,
  "load_pretrained_checkpoint": false,
  "lora": true,
  "lora_compressor": false,
  "lora_r": 16,
  "lora_r_compressor": 16,
  "max_new_tokens": 128,
  "model_type": "UniRAG",
  "optimize_mem_tokens": true,
  "pad_token_id": 2,
  "pure_inference": false,
  "quantization": "no",
  "sep": true,
  "stage2_retrieval_top_n": 1,
  "training_form": "both_separately",
  "training_stage": "stage2",
  "transformers_version": "4.56.2"
}
Initializing model from trained checkpoint: UniRAGConfig {
  "_attn_implementation_autoset": true,
  "ae_mode": "token",
  "attn_implementation": null,
  "auto_map": {
    "AutoConfig": "modeling_unirag.UniRAGConfig",
    "AutoModel": "modeling_unirag.UniRAG"
  },
  "compr_base_model_name": "/mnt/ceph_rbd/model/Mistral-7B-Instruct-v0.2",
  "compr_every_n_layer": null,
  "compr_linear_type": "concat",
  "compr_mlp_hidden_dim": 8096,
  "compr_model_name": null,
  "compr_n_layers": 5,
  "compr_rate": 32,
  "compr_rms_norm": false,
  "compr_use_mlp": false,
  "decoder_model_name": "/mnt/ceph_rbd/model/Mistral-7B-Instruct-v0.2",
  "device_map": null,
  "different_mem_tokens": true,
  "doc_max_length": 256,
  "generation_top_k": 5,
  "kbtc_training": false,
  "load_adapters": false,
  "load_pretrained_checkpoint": false,
  "lora": true,
  "lora_compressor": false,
  "lora_r": 16,
  "lora_r_compressor": 16,
  "max_new_tokens": 128,
  "model_type": "UniRAG",
  "optimize_mem_tokens": true,
  "pad_token_id": 2,
  "pure_inference": false,
  "quantization": "no",
  "sep": true,
  "stage2_retrieval_top_n": 1,
  "training_form": "both_separately",
  "training_stage": "stage2",
  "transformers_version": "4.56.2"
}
Initializing model from trained checkpoint: UniRAGConfig {
  "_attn_implementation_autoset": true,
  "ae_mode": "token",
  "attn_implementation": null,
  "auto_map": {
    "AutoConfig": "modeling_unirag.UniRAGConfig",
    "AutoModel": "modeling_unirag.UniRAG"
  },
  "compr_base_model_name": "/mnt/ceph_rbd/model/Mistral-7B-Instruct-v0.2",
  "compr_every_n_layer": null,
  "compr_linear_type": "concat",
  "compr_mlp_hidden_dim": 8096,
  "compr_model_name": null,
  "compr_n_layers": 5,
  "compr_rate": 32,
  "compr_rms_norm": false,
  "compr_use_mlp": false,
  "decoder_model_name": "/mnt/ceph_rbd/model/Mistral-7B-Instruct-v0.2",
  "device_map": null,
  "different_mem_tokens": true,
  "doc_max_length": 256,
  "generation_top_k": 5,
  "kbtc_training": false,
  "load_adapters": false,
  "load_pretrained_checkpoint": false,
  "lora": true,
  "lora_compressor": false,
  "lora_r": 16,
  "lora_r_compressor": 16,
  "max_new_tokens": 128,
  "model_type": "UniRAG",
  "optimize_mem_tokens": true,
  "pad_token_id": 2,
  "pure_inference": false,
  "quantization": "no",
  "sep": true,
  "stage2_retrieval_top_n": 1,
  "training_form": "both_separately",
  "training_stage": "stage2",
  "transformers_version": "4.56.2"
}

Initializing model from trained checkpoint: UniRAGConfig {
  "_attn_implementation_autoset": true,
  "ae_mode": "token",
  "attn_implementation": null,
  "auto_map": {
    "AutoConfig": "modeling_unirag.UniRAGConfig",
    "AutoModel": "modeling_unirag.UniRAG"
  },
  "compr_base_model_name": "/mnt/ceph_rbd/model/Mistral-7B-Instruct-v0.2",
  "compr_every_n_layer": null,
  "compr_linear_type": "concat",
  "compr_mlp_hidden_dim": 8096,
  "compr_model_name": null,
  "compr_n_layers": 5,
  "compr_rate": 32,
  "compr_rms_norm": false,
  "compr_use_mlp": false,
  "decoder_model_name": "/mnt/ceph_rbd/model/Mistral-7B-Instruct-v0.2",
  "device_map": null,
  "different_mem_tokens": true,
  "doc_max_length": 256,
  "generation_top_k": 5,
  "kbtc_training": false,
  "load_adapters": false,
  "load_pretrained_checkpoint": false,
  "lora": true,
  "lora_compressor": false,
  "lora_r": 16,
  "lora_r_compressor": 16,
  "max_new_tokens": 128,
  "model_type": "UniRAG",
  "optimize_mem_tokens": true,
  "pad_token_id": 2,
  "pure_inference": false,
  "quantization": "no",
  "sep": true,
  "stage2_retrieval_top_n": 1,
  "training_form": "both_separately",
  "training_stage": "stage2",
  "transformers_version": "4.56.2"
}



`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:04<00:09,  4.94s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:04<00:09,  4.93s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:04<00:09,  4.93s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:04<00:09,  4.93s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:09<00:04,  4.43s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:09<00:04,  4.43s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:09<00:04,  4.43s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:09<00:04,  4.43s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:13<00:00,  4.26s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:13<00:00,  4.26s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:13<00:00,  4.26s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:13<00:00,  4.26s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:13<00:00,  4.35s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:13<00:00,  4.35s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:13<00:00,  4.35s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:13<00:00,  4.36s/it]
Base decoder parameters: 7241732096Base decoder parameters: 7241732096
Base decoder parameters: 7241732096
Base decoder parameters: 7241732096

Model adapter keys: []
Model adapter keys: []
Model adapter keys: []Model adapter keys: []

Memory token count: 8
Memory token count: 8
Memory token count: 8
Memory token count: 8
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint adapter: decoder_adapter
Loading checkpoint adapter: decoder_adapter
Loading checkpoint adapter: decoder_adapter
Loading checkpoint adapter: decoder_adapter
Loading checkpoint adapter: encoder_adapter
Loading checkpoint adapter: encoder_adapter
Loading checkpoint adapter: encoder_adapter
Loading checkpoint adapter: encoder_adapter
/mnt/ceph_rbd/conda_env/cprag/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/mnt/ceph_rbd/conda_env/cprag/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/mnt/ceph_rbd/conda_env/cprag/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/mnt/ceph_rbd/conda_env/cprag/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
Loading checkpoint adapter: query_reasoner_adapter
Loading checkpoint adapter: query_reasoner_adapter
Loading checkpoint adapter: query_reasoner_adapter
Loading checkpoint adapter: query_reasoner_adapter
Loaded query_reasoner_adapter from stage 1 compressor checkpoint
Loaded query_reasoner_adapter from stage 1 compressor checkpoint
Loaded query_reasoner_adapter from stage 1 compressor checkpoint
Loaded query_reasoner_adapter from stage 1 compressor checkpoint
æ‰“å°æ•°æ® .jsonl /mnt/ceph_rbd/comp_rag/unirag/debug_data/end_to_end_data.jsonl
æ‰“å°æ•°æ® .jsonl /mnt/ceph_rbd/comp_rag/unirag/debug_data/end_to_end_data.jsonl
UniRAG(
  (decoder): MistralForCausalLM(
    (model): MistralModel(
      (embed_tokens): Embedding(32011, 4096)
      (layers): ModuleList(
        (0-31): 32 x MistralDecoderLayer(
          (self_attn): MistralAttention(
            (q_proj): lora.Linear(
              (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
              (lora_dropout): ModuleDict(
                (decoder_adapter): Dropout(p=0.1, inplace=False)
                (encoder_adapter): Dropout(p=0.1, inplace=False)
                (query_reasoner_adapter): Dropout(p=0.1, inplace=False)
              )
              (lora_A): ModuleDict(
                (decoder_adapter): Linear(in_features=4096, out_features=16, bias=False)
                (encoder_adapter): Linear(in_features=4096, out_features=16, bias=False)
                (query_reasoner_adapter): Linear(in_features=4096, out_features=16, bias=False)
              )
              (lora_B): ModuleDict(
                (decoder_adapter): Linear(in_features=16, out_features=4096, bias=False)
                (encoder_adapter): Linear(in_features=16, out_features=4096, bias=False)
                (query_reasoner_adapter): Linear(in_features=16, out_features=4096, bias=False)
              )
              (lora_embedding_A): ParameterDict()
              (lora_embedding_B): ParameterDict()
              (lora_magnitude_vector): ModuleDict()
            )
            (k_proj): lora.Linear(
              (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
              (lora_dropout): ModuleDict(
                (decoder_adapter): Dropout(p=0.1, inplace=False)
                (encoder_adapter): Dropout(p=0.1, inplace=False)
                (query_reasoner_adapter): Dropout(p=0.1, inplace=False)
              )
              (lora_A): ModuleDict(
                (decoder_adapter): Linear(in_features=4096, out_features=16, bias=False)
                (encoder_adapter): Linear(in_features=4096, out_features=16, bias=False)
                (query_reasoner_adapter): Linear(in_features=4096, out_features=16, bias=False)
              )
              (lora_B): ModuleDict(
                (decoder_adapter): Linear(in_features=16, out_features=1024, bias=False)
                (encoder_adapter): Linear(in_features=16, out_features=1024, bias=False)
                (query_reasoner_adapter): Linear(in_features=16, out_features=1024, bias=False)
              )
              (lora_embedding_A): ParameterDict()
              (lora_embedding_B): ParameterDict()
              (lora_magnitude_vector): ModuleDict()
            )
            (v_proj): lora.Linear(
              (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
              (lora_dropout): ModuleDict(
                (decoder_adapter): Dropout(p=0.1, inplace=False)
                (encoder_adapter): Dropout(p=0.1, inplace=False)
                (query_reasoner_adapter): Dropout(p=0.1, inplace=False)
              )
              (lora_A): ModuleDict(
                (decoder_adapter): Linear(in_features=4096, out_features=16, bias=False)
                (encoder_adapter): Linear(in_features=4096, out_features=16, bias=False)
                (query_reasoner_adapter): Linear(in_features=4096, out_features=16, bias=False)
              )
              (lora_B): ModuleDict(
                (decoder_adapter): Linear(in_features=16, out_features=1024, bias=False)
                (encoder_adapter): Linear(in_features=16, out_features=1024, bias=False)
                (query_reasoner_adapter): Linear(in_features=16, out_features=1024, bias=False)
              )
              (lora_embedding_A): ParameterDict()
              (lora_embedding_B): ParameterDict()
              (lora_magnitude_vector): ModuleDict()
            )
            (o_proj): lora.Linear(
              (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
              (lora_dropout): ModuleDict(
                (decoder_adapter): Dropout(p=0.1, inplace=False)
                (encoder_adapter): Dropout(p=0.1, inplace=False)
                (query_reasoner_adapter): Dropout(p=0.1, inplace=False)
              )
              (lora_A): ModuleDict(
                (decoder_adapter): Linear(in_features=4096, out_features=16, bias=False)
                (encoder_adapter): Linear(in_features=4096, out_features=16, bias=False)
                (query_reasoner_adapter): Linear(in_features=4096, out_features=16, bias=False)
              )
              (lora_B): ModuleDict(
                (decoder_adapter): Linear(in_features=16, out_features=4096, bias=False)
                (encoder_adapter): Linear(in_features=16, out_features=4096, bias=False)
                (query_reasoner_adapter): Linear(in_features=16, out_features=4096, bias=False)
              )
              (lora_embedding_A): ParameterDict()
              (lora_embedding_B): ParameterDict()
              (lora_magnitude_vector): ModuleDict()
            )
          )
          (mlp): MistralMLP(
            (gate_proj): lora.Linear(
              (base_layer): Linear(in_features=4096, out_features=14336, bias=False)
              (lora_dropout): ModuleDict(
                (decoder_adapter): Dropout(p=0.1, inplace=False)
                (encoder_adapter): Dropout(p=0.1, inplace=False)
                (query_reasoner_adapter): Dropout(p=0.1, inplace=False)
              )
              (lora_A): ModuleDict(
                (decoder_adapter): Linear(in_features=4096, out_features=16, bias=False)
                (encoder_adapter): Linear(in_features=4096, out_features=16, bias=False)
                (query_reasoner_adapter): Linear(in_features=4096, out_features=16, bias=False)
              )
              (lora_B): ModuleDict(
                (decoder_adapter): Linear(in_features=16, out_features=14336, bias=False)
                (encoder_adapter): Linear(in_features=16, out_features=14336, bias=False)
                (query_reasoner_adapter): Linear(in_features=16, out_features=14336, bias=False)
              )
              (lora_embedding_A): ParameterDict()
              (lora_embedding_B): ParameterDict()
              (lora_magnitude_vector): ModuleDict()
            )
            (up_proj): lora.Linear(
              (base_layer): Linear(in_features=4096, out_features=14336, bias=False)
              (lora_dropout): ModuleDict(
                (decoder_adapter): Dropout(p=0.1, inplace=False)
                (encoder_adapter): Dropout(p=0.1, inplace=False)
                (query_reasoner_adapter): Dropout(p=0.1, inplace=False)
              )
              (lora_A): ModuleDict(
                (decoder_adapter): Linear(in_features=4096, out_features=16, bias=False)
                (encoder_adapter): Linear(in_features=4096, out_features=16, bias=False)
                (query_reasoner_adapter): Linear(in_features=4096, out_features=16, bias=False)
              )
              (lora_B): ModuleDict(
                (decoder_adapter): Linear(in_features=16, out_features=14336, bias=False)
                (encoder_adapter): Linear(in_features=16, out_features=14336, bias=False)
                (query_reasoner_adapter): Linear(in_features=16, out_features=14336, bias=False)
              )
              (lora_embedding_A): ParameterDict()
              (lora_embedding_B): ParameterDict()
              (lora_magnitude_vector): ModuleDict()
            )
            (down_proj): lora.Linear(
              (base_layer): Linear(in_features=14336, out_features=4096, bias=False)
              (lora_dropout): ModuleDict(
                (decoder_adapter): Dropout(p=0.1, inplace=False)
                (encoder_adapter): Dropout(p=0.1, inplace=False)
                (query_reasoner_adapter): Dropout(p=0.1, inplace=False)
              )
              (lora_A): ModuleDict(
                (decoder_adapter): Linear(in_features=14336, out_features=16, bias=False)
                (encoder_adapter): Linear(in_features=14336, out_features=16, bias=False)
                (query_reasoner_adapter): Linear(in_features=14336, out_features=16, bias=False)
              )
              (lora_B): ModuleDict(
                (decoder_adapter): Linear(in_features=16, out_features=4096, bias=False)
                (encoder_adapter): Linear(in_features=16, out_features=4096, bias=False)
                (query_reasoner_adapter): Linear(in_features=16, out_features=4096, bias=False)
              )
              (lora_embedding_A): ParameterDict()
              (lora_embedding_B): ParameterDict()
              (lora_magnitude_vector): ModuleDict()
            )
            (act_fn): SiLU()
          )
          (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)
          (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)
        )
      )
      (norm): MistralRMSNorm((4096,), eps=1e-05)
      (rotary_emb): MistralRotaryEmbedding()
    )
    (lm_head): Linear(in_features=4096, out_features=32011, bias=False)
  )
)
dataset: /mnt/ceph_rbd/comp_rag/unirag/debug_data/end_to_end_data.jsonl
æ‰“å°æ•°æ® .jsonl /mnt/ceph_rbd/comp_rag/unirag/debug_data/end_to_end_data.jsonl
æ‰“å°æ•°æ® .jsonl /mnt/ceph_rbd/comp_rag/unirag/debug_data/end_to_end_data.jsonl
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 808 examples [00:00, 7870.19 examples/s]Generating train split: 1000 examples [00:00, 8633.20 examples/s]
loaded /mnt/ceph_rbd/comp_rag/unirag/debug_data/end_to_end_data.jsonl with data_files=/mnt/ceph_rbd/comp_rag/unirag/debug_data/end_to_end_data.jsonl
[Dataset({
    features: ['question', 'answer', 'docs', 'pos_index'],
    num_rows: 500
})]
Map (num_proc=8):   0%|          | 0/500 [00:00<?, ? examples/s]Map (num_proc=8):   0%|          | 0/500 [00:00<?, ? examples/s]Map (num_proc=8):   0%|          | 0/500 [00:00<?, ? examples/s]Map (num_proc=8):   0%|          | 0/500 [00:00<?, ? examples/s]Map (num_proc=8):   9%|â–‰         | 45/500 [00:00<00:04, 103.75 examples/s]Map (num_proc=8):  13%|â–ˆâ–Ž        | 63/500 [00:00<00:03, 120.52 examples/s]Map (num_proc=8):  13%|â–ˆâ–Ž        | 63/500 [00:00<00:03, 121.18 examples/s]Map (num_proc=8):  13%|â–ˆâ–Ž        | 63/500 [00:00<00:04, 101.22 examples/s]Map (num_proc=8):  20%|â–ˆâ–ˆ        | 100/500 [00:00<00:02, 170.52 examples/s]Map (num_proc=8):  13%|â–ˆâ–Ž        | 63/500 [00:00<00:04, 91.96 examples/s]Map (num_proc=8):  34%|â–ˆâ–ˆâ–ˆâ–      | 171/500 [00:00<00:01, 264.68 examples/s]Map (num_proc=8):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 252/500 [00:00<00:00, 390.05 examples/s]Map (num_proc=8):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 252/500 [00:00<00:00, 342.94 examples/s]Map (num_proc=8):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 376/500 [00:00<00:00, 501.90 examples/s]Map (num_proc=8):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 314/500 [00:01<00:00, 375.88 examples/s]Map (num_proc=8):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 312/500 [00:01<00:00, 355.97 examples/s]Map (num_proc=8):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 438/500 [00:01<00:00, 650.72 examples/s]Map (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:01<00:00, 606.94 examples/s]Map (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:01<00:00, 433.99 examples/s]
Map (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:01<00:00, 433.69 examples/s]
Map (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:01<00:00, 430.30 examples/s]
Map (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:01<00:00, 406.91 examples/s]
/mnt/ceph_rbd/conda_env/cprag/lib/python3.10/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Train epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Train step of epoch 0:   0%|          | 0/62 [00:00<?, ?it/s][A
Train step of epoch 0:   2%|â–         | 1/62 [00:05<05:33,  5.47s/it][A
Train step of epoch 0:   3%|â–Ž         | 2/62 [00:07<03:28,  3.48s/it][A
Train step of epoch 0:   5%|â–         | 3/62 [00:09<02:45,  2.80s/it][A
Train step of epoch 0:   5%|â–         | 3/62 [00:12<02:45,  2.80s/it, loss=8.29, mse_loss=13.1, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:   6%|â–‹         | 4/62 [00:12<02:41,  2.78s/it, loss=8.29, mse_loss=13.1, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:   8%|â–Š         | 5/62 [00:14<02:22,  2.49s/it, loss=8.29, mse_loss=13.1, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  10%|â–‰         | 6/62 [00:16<02:10,  2.32s/it, loss=8.29, mse_loss=13.1, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  11%|â–ˆâ–        | 7/62 [00:18<02:01,  2.21s/it, loss=8.29, mse_loss=13.1, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  11%|â–ˆâ–        | 7/62 [00:20<02:01,  2.21s/it, loss=7.08, mse_loss=13.4, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  13%|â–ˆâ–Ž        | 8/62 [00:20<01:56,  2.15s/it, loss=7.08, mse_loss=13.4, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  15%|â–ˆâ–        | 9/62 [00:22<01:51,  2.10s/it, loss=7.08, mse_loss=13.4, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  16%|â–ˆâ–Œ        | 10/62 [00:24<01:47,  2.07s/it, loss=7.08, mse_loss=13.4, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  18%|â–ˆâ–Š        | 11/62 [00:26<01:44,  2.05s/it, loss=7.08, mse_loss=13.4, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  18%|â–ˆâ–Š        | 11/62 [00:28<01:44,  2.05s/it, loss=7, mse_loss=13, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0]     [A
Train step of epoch 0:  19%|â–ˆâ–‰        | 12/62 [00:28<01:42,  2.04s/it, loss=7, mse_loss=13, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  21%|â–ˆâ–ˆ        | 13/62 [00:30<01:39,  2.03s/it, loss=7, mse_loss=13, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 14/62 [00:32<01:36,  2.02s/it, loss=7, mse_loss=13, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  24%|â–ˆâ–ˆâ–       | 15/62 [00:34<01:34,  2.02s/it, loss=7, mse_loss=13, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  24%|â–ˆâ–ˆâ–       | 15/62 [00:36<01:34,  2.02s/it, loss=6.65, mse_loss=13.4, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 16/62 [00:36<01:32,  2.02s/it, loss=6.65, mse_loss=13.4, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  27%|â–ˆâ–ˆâ–‹       | 17/62 [00:38<01:30,  2.01s/it, loss=6.65, mse_loss=13.4, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  29%|â–ˆâ–ˆâ–‰       | 18/62 [00:40<01:28,  2.01s/it, loss=6.65, mse_loss=13.4, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 19/62 [00:42<01:26,  2.01s/it, loss=6.65, mse_loss=13.4, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 19/62 [00:44<01:26,  2.01s/it, loss=4.6, mse_loss=13.2, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0] [A
Train step of epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 20/62 [00:44<01:24,  2.01s/it, loss=4.6, mse_loss=13.2, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 21/62 [00:46<01:22,  2.01s/it, loss=4.6, mse_loss=13.2, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 22/62 [00:48<01:20,  2.02s/it, loss=4.6, mse_loss=13.2, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 23/62 [00:50<01:18,  2.02s/it, loss=4.6, mse_loss=13.2, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 23/62 [00:52<01:18,  2.02s/it, loss=5.14, mse_loss=13.2, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 24/62 [00:52<01:17,  2.03s/it, loss=5.14, mse_loss=13.2, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 25/62 [00:54<01:14,  2.02s/it, loss=5.14, mse_loss=13.2, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/62 [00:56<01:12,  2.02s/it, loss=5.14, mse_loss=13.2, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 27/62 [00:58<01:10,  2.02s/it, loss=5.14, mse_loss=13.2, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 27/62 [01:00<01:10,  2.02s/it, loss=4.78, mse_loss=13.3, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 28/62 [01:00<01:08,  2.02s/it, loss=4.78, mse_loss=13.3, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 29/62 [01:02<01:06,  2.02s/it, loss=4.78, mse_loss=13.3, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 30/62 [01:04<01:04,  2.01s/it, loss=4.78, mse_loss=13.3, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 31/62 [01:06<01:02,  2.01s/it, loss=4.78, mse_loss=13.3, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 31/62 [01:08<01:02,  2.01s/it, loss=4.36, mse_loss=13.2, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/62 [01:08<01:00,  2.01s/it, loss=4.36, mse_loss=13.2, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 33/62 [01:10<00:58,  2.01s/it, loss=4.36, mse_loss=13.2, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 34/62 [01:12<00:56,  2.01s/it, loss=4.36, mse_loss=13.2, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 35/62 [01:14<00:54,  2.01s/it, loss=4.36, mse_loss=13.2, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 35/62 [01:16<00:54,  2.01s/it, loss=4.86, mse_loss=13.3, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 36/62 [01:16<00:52,  2.02s/it, loss=4.86, mse_loss=13.3, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 37/62 [01:18<00:50,  2.02s/it, loss=4.86, mse_loss=13.3, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/62 [01:20<00:48,  2.02s/it, loss=4.86, mse_loss=13.3, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 39/62 [01:22<00:46,  2.01s/it, loss=4.86, mse_loss=13.3, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 39/62 [01:24<00:46,  2.01s/it, loss=3.6, mse_loss=13.4, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0] [A
Train step of epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 40/62 [01:24<00:44,  2.02s/it, loss=3.6, mse_loss=13.4, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 41/62 [01:26<00:42,  2.02s/it, loss=3.6, mse_loss=13.4, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 42/62 [01:28<00:40,  2.01s/it, loss=3.6, mse_loss=13.4, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 43/62 [01:30<00:38,  2.01s/it, loss=3.6, mse_loss=13.4, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 43/62 [01:32<00:38,  2.01s/it, loss=3.54, mse_loss=13.5, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 44/62 [01:32<00:36,  2.02s/it, loss=3.54, mse_loss=13.5, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 45/62 [01:34<00:34,  2.02s/it, loss=3.54, mse_loss=13.5, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 46/62 [01:36<00:32,  2.02s/it, loss=3.54, mse_loss=13.5, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 47/62 [01:38<00:30,  2.02s/it, loss=3.54, mse_loss=13.5, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 47/62 [01:40<00:30,  2.02s/it, loss=3.65, mse_loss=13.1, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 48/62 [01:40<00:28,  2.02s/it, loss=3.65, mse_loss=13.1, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 49/62 [01:42<00:26,  2.02s/it, loss=3.65, mse_loss=13.1, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 50/62 [01:44<00:24,  2.02s/it, loss=3.65, mse_loss=13.1, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 51/62 [01:46<00:22,  2.01s/it, loss=3.65, mse_loss=13.1, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 51/62 [01:48<00:22,  2.01s/it, loss=3.56, mse_loss=13.3, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 52/62 [01:48<00:20,  2.02s/it, loss=3.56, mse_loss=13.3, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 53/62 [01:50<00:18,  2.02s/it, loss=3.56, mse_loss=13.3, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 54/62 [01:52<00:16,  2.02s/it, loss=3.56, mse_loss=13.3, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 55/62 [01:54<00:14,  2.02s/it, loss=3.56, mse_loss=13.3, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 55/62 [01:57<00:14,  2.02s/it, loss=3.35, mse_loss=13.1, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 56/62 [01:57<00:12,  2.03s/it, loss=3.35, mse_loss=13.1, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 57/62 [01:59<00:10,  2.04s/it, loss=3.35, mse_loss=13.1, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 58/62 [02:01<00:08,  2.04s/it, loss=3.35, mse_loss=13.1, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 59/62 [02:03<00:06,  2.04s/it, loss=3.35, mse_loss=13.1, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 59/62 [02:05<00:06,  2.04s/it, loss=3.03, mse_loss=13.2, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 60/62 [02:19<00:12,  6.31s/it, loss=3.03, mse_loss=13.2, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 61/62 [02:21<00:05,  5.04s/it, loss=3.03, mse_loss=13.2, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][A
Train step of epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [02:23<00:00,  4.13s/it, loss=3.03, mse_loss=13.2, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0][ATrain epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:23<00:00, 143.54s/it]Train epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:23<00:00, 143.54s/it]
Train step of epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [02:23<00:00,  2.32s/it, loss=3.03, mse_loss=13.2, lr=5e-6, retrieval_recall_1=0, retrieval_recall_3=0, retrieval_recall_5=0, retrieval_precision_1=0, retrieval_precision_3=0, retrieval_precision_5=0]
Training completed successfully!
Training completed successfully!
Training completed successfully!
Training completed successfully!
[rank3]:[W1028 21:54:13.489618291 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1028 21:54:13.489665500 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank2]:[W1028 21:54:13.509674053 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank1]:[W1028 21:54:13.525928345 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
+ cp ../openrlhf/models/modeling_unirag.py /mnt/ceph_rbd/comp_rag/unirag/debug_data/train_checkpoint/unirag_stage2_debug
